{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source of text:\n",
    "#https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
    "sentences = []\n",
    "with open('genia4ertraining/tokenized_sentences.txt','r') as f:\n",
    "    lines = f.read().rstrip().split('\\n')\n",
    "    for line in lines:\n",
    "        if len(line.split())==0:\n",
    "            continue\n",
    "        Text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434\n"
     ]
    }
   ],
   "source": [
    "bowlist = []\n",
    "for para in Text:\n",
    "    bowlist.append(para.split(\" \"))\n",
    "vocabulary = list(set().union(*bowlist))\n",
    "processed_text = bowlist[0]+bowlist[1]+bowlist[2]+bowlist[3]+bowlist[4]+bowlist[5]+bowlist[6]+bowlist[7]+bowlist[8]+bowlist[9]\n",
    "print(len(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 3\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(processed_text)-window_size)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = processed_text[window_start:window_end]\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    for j in range(0,vocab_len):\n",
    "        inout[i]+=weighted_edge[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converging at iteration 33....\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 50\n",
    "d=0.85\n",
    "threshold = 0.0001 #convergence threshold\n",
    "\n",
    "for iter in range(0,MAX_ITERATIONS):\n",
    "    prev_score = np.copy(score)\n",
    "    \n",
    "    for i in range(0,vocab_len):\n",
    "        \n",
    "        summation = 0\n",
    "        for j in range(0,vocab_len):\n",
    "            if weighted_edge[i][j] != 0:\n",
    "                summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "                \n",
    "        score[i] = (1-d) + d*(summation)\n",
    "    \n",
    "    if np.sum(np.fabs(prev_score-score)) <= threshold: #convergence condition\n",
    "        print (\"Converging at iteration \"+str(iter)+\"....\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e5b350579564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnewlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(len(newlist))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnewlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "newlist = list(zip(vocabulary, score))\n",
    "#print(len(newlist))\n",
    "newlist = sorted(newlist, key=lambda tup: tup[1],reverse=True)\n",
    "preds = []\n",
    "for i in range(0,len(newlist)):\n",
    "    preds.append(list(newlist[i]))\n",
    "#newdict = \n",
    "\n",
    "count = 0\n",
    "for i in range(0,len(preds)):\n",
    "    #print (\"Score of \"+newlist[i][0]+\": \"+str(newlist[i][1]))\n",
    "    if count<358:\n",
    "        preds[i][1] = 1\n",
    "    else:\n",
    "        preds[i][1] = 0\n",
    "    count+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = []\n",
    "with open('4ersample.txt','r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for i in lines:\n",
    "        element = i.strip().split('\\t')\n",
    "        wrds.append(element)\n",
    "        \n",
    "predsfinal = sorted(preds, key=lambda tup: tup[0])\n",
    "testfinal = sorted(wrds, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.5605\n",
      "f1 0.3084\n",
      "[[340  95]\n",
      " [228  72]]\n",
      "prec 0.4311\n",
      "rec 0.24\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "test = []\n",
    "count = 0\n",
    "for tup in predsfinal:\n",
    "    if count<735:\n",
    "        preds.append(tup[1])\n",
    "        count+=1\n",
    "for tup in testfinal:\n",
    "    test.append(int(tup[1]))\n",
    "\n",
    "preds = np.array(preds)\n",
    "test = np.array(test)\n",
    "print('acc',round(accuracy_score(preds,test),4))\n",
    "print('f1',round(f1_score(preds,test),4))\n",
    "print(confusion_matrix(preds,test))\n",
    "print('prec',round(precision_score(preds,test),4))\n",
    "print('rec',round(recall_score(preds,test),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Phrases (Candidate Keyphrases): \n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "phrases = []\n",
    "with open('long_stopwords.txt','r') as fin:\n",
    "    stopwords = fin.read().splitlines()\n",
    "phrase = \" \"\n",
    "for word in processed_text:\n",
    "    if word in stopwords:\n",
    "        if phrase!= \" \":\n",
    "            phrases.append(str(phrase).strip().split())\n",
    "        phrase = \" \"\n",
    "    \n",
    "    elif word not in stopwords:\n",
    "        phrase+=str(word)\n",
    "        phrase+=\" \"\n",
    "\n",
    "print (\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "print (phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['compatibility'], ['system'], ['linear', 'constraint'], ['set'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['type'], ['constructing'], ['minimal', 'supporting', 'set'], ['solving'], ['mixed', 'type']]\n"
     ]
    }
   ],
   "source": [
    "unique_phrases = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if phrase not in unique_phrases:\n",
    "        unique_phrases.append(phrase)\n",
    "\n",
    "print \"Unique Phrases (Candidate Keyphrases): \\n\"\n",
    "print unique_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinning the list of candidate-keyphrases.\n",
    "\n",
    "Removing single word keyphrases-candidates that are present multi-word alternatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinned Unique Phrases (Candidate Keyphrases): \n",
      "\n",
      "[['compatibility'], ['system'], ['linear', 'constraint'], ['natural', 'number'], ['criterion'], ['linear', 'diophantine', 'equation'], ['strict', 'inequations'], ['nonstrict', 'inequations'], ['upper', 'bound'], ['component'], ['minimal', 'set'], ['solution'], ['algorithm'], ['construction'], ['minimal', 'generating', 'set'], ['constructing'], ['minimal', 'supporting', 'set'], ['solving'], ['mixed', 'type']]\n"
     ]
    }
   ],
   "source": [
    "for word in vocabulary:\n",
    "    #print word\n",
    "    for phrase in unique_phrases:\n",
    "        if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "            #if len(phrase)>1 then the current phrase is multi-worded.\n",
    "            #if the word in vocabulary is present in unique_phrases as a single-word-phrase\n",
    "            # and at the same time present as a word within a multi-worded phrase,\n",
    "            # then I will remove the single-word-phrase from the list.\n",
    "            unique_phrases.remove([word])\n",
    "            \n",
    "print \"Thinned Unique Phrases (Candidate Keyphrases): \\n\"\n",
    "print unique_phrases    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Keyphrases\n",
    "\n",
    "Scoring the phrases (candidate keyphrases) and building up a list of keyphrases\\keywords\n",
    "by listing untokenized versions of tokenized phrases\\candidate-keyphrases.\n",
    "Phrases are scored by adding the score of their members (words\\text-units that were ranked by the graph algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: 'compatibility', Score: 0.944583714008\n",
      "Keyword: 'system', Score: 2.12031626701\n",
      "Keyword: 'linear constraint', Score: 1.94610738754\n",
      "Keyword: 'natural number', Score: 1.37661552429\n",
      "Keyword: 'criterion', Score: 1.2255872488\n",
      "Keyword: 'linear diophantine equation', Score: 2.83080631495\n",
      "Keyword: 'strict inequations', Score: 2.13201224804\n",
      "Keyword: 'nonstrict inequations', Score: 2.135455966\n",
      "Keyword: 'upper bound', Score: 1.60279768705\n",
      "Keyword: 'component', Score: 0.737640619278\n",
      "Keyword: 'minimal set', Score: 4.05876886845\n",
      "Keyword: 'solution', Score: 1.68319940567\n",
      "Keyword: 'algorithm', Score: 1.19365406036\n",
      "Keyword: 'construction', Score: 0.659808635712\n",
      "Keyword: 'minimal generating set', Score: 4.71141409874\n",
      "Keyword: 'constructing', Score: 0.66728836298\n",
      "Keyword: 'minimal supporting set', Score: 4.71247345209\n",
      "Keyword: 'solving', Score: 0.642318367958\n",
      "Keyword: 'mixed type', Score: 1.31682945788\n"
     ]
    }
   ],
   "source": [
    "phrase_scores = []\n",
    "keywords = []\n",
    "for phrase in unique_phrases:\n",
    "    phrase_score=0\n",
    "    keyword = ''\n",
    "    for word in phrase:\n",
    "        keyword += str(word)\n",
    "        keyword += \" \"\n",
    "        phrase_score+=score[vocabulary.index(word)]\n",
    "    phrase_scores.append(phrase_score)\n",
    "    keywords.append(keyword.strip())\n",
    "\n",
    "i=0\n",
    "for keyword in keywords:\n",
    "    print \"Keyword: '\"+str(keyword)+\"', Score: \"+str(phrase_scores[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Keyphrases\n",
    "\n",
    "Ranking keyphrases based on their calculated scores. Displaying top keywords_num no. of keyphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "\n",
    "keywords_num = 10\n",
    "\n",
    "print \"Keywords:\\n\"\n",
    "\n",
    "for i in xrange(0,keywords_num):\n",
    "    print str(keywords[sorted_index[i]])+\", \","
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
